%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
% Johan Bos (johan.bos@rug.nl)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
%BCOR5mm, % Binding correction
] {book}% {scrartcl}
\usepackage{eurosym}
\usepackage{booktabs}
\usepackage{csvsimple,longtable}
\setcounter{secnumdepth}{5}
\input{structure.tex} % Include the structure.tex file which specified the document structure and layout


%----------------------------------------------------------------------------------------
%	HYPHENATION
%----------------------------------------------------------------------------------------

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether



%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{title}} % The article title

\author{\spacedlowsmallcaps{author}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

%\renewcommand{\chaptermark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
%\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\hypersetup{pageanchor=false}
\begin{titlepage}
\thispagestyle{empty}
\begin{figure}[h!] %  figure placement: here, top, bottom, or page
\includegraphics[width=4in]{ruglogo} 
\end{figure}

\begin{center}
\vspace{30 mm}
\begingroup \linespread{1,75} \selectfont 
\textsc{\LARGE Detecting Income Level of Dutch Twitter Users using Stylometric Features}\\
% \textsc{\Large And this is an optional subtitle}
[1,5cm]
\endgroup

L\'eon Melein\\[2,5cm]

\end{center}
\vfill
\textbf{Bachelor thesis - Concept version}\\  %\textbf{Master thesis}\\
Information Science\\  %Information Science\\
L\'eon Melein\\
S2580861\\
\today\\
Supervisor: B. Plank
\end{titlepage}
\hypersetup{pageanchor=true}



%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\pagenumbering{roman}
\chapter*{Abstract}
\markboth{Abstract}{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
Income prediction is a relatively undiscovered aspect of author profiling. Early research on English \citep{flekova} linking Twitter users to occupations and their respective average incomes, obtained promising results. There is no comparable research for Dutch speakers yet. In this thesis, we explore to what extent author profiling can predict the income level of Dutch users.

We do so by creating a dataset of 2000 Twitter users. These are divided into two income classes as there currently is no complete income data available for individual occupations in The Netherlands. We use \textit{distant supervision} to annotate users with their occupational class and their income. We then extract a number of surface, readability and n-gram features from the users' posts. Using logistic regression, we try to classify the users on their income class with those features.

After testing various feature groupings, the classifier proved to be the most robust with uni-, bi- and trigram features, reaching an F1-score of 0.72. Although this indicates that profiling can predict a user's income class to a very large extent, this can only be seen as a first indication as the scope of this study is limited. With clear directions for future improvement, we hope that this study may be a stepping stone towards the prediction of individual incomes for Dutch authors.

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------
\clearpage
\setcounter{tocdepth}{3} % Set the depth of the table of contents to show sections and subsections only
\tableofcontents % Print the table of contents

%\listoffigures % Print the list of figures (optional, only if you have many figures)

%\listoftables % Print the list of tables (optional, only if you have many tables)

%\lstlistoflistings



%----------------------------------------------------------------------------------------
%	Preface
%----------------------------------------------------------------------------------------

\chapter*{Preface}
\markboth{Preface}{Preface}
\addcontentsline{toc}{chapter}{Preface}

I would like to take this opportunity to thank a number of people who have been instrumental to finishing my thesis. 
First of all, my supervisor Barbara. Thanks to her relentless support and guidance, I was able to tackle my research question. Even when my research proved more challenging than expected, she helped me to tackle the problems step by step. 

I also want to thank fellow student Reinard van Dalen for his advice and support. As our thesis subjects partly overlap, it really helped to look at certain problems together and then solve them in the most efficient way possible. And a final thank you to Stijn Eikelboom, who proofread my thesis.


Their support could obviously not prevent the toll my thesis took on my keyboard. It broke down while I was writing the first chapters. It turned out to be a worthwhile sacrifice for completing my thesis. 




%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\chapter{Introduction}
\pagenumbering{arabic}
As we rely more and more on the internet and its applications in our daily lives, the opportunities for author profiling continue to grow. Social media services, and the text-based communications they facilitate, provide an ever-growing corpus of texts, which are linked to their author. Furthermore, these services provide additional self-disclosed information about the authors like education, occupation and relationships. If we use this data the right way, we can perform research that simply wasn't possible before, at least not on this scale \citep{sloan}.


An aspect of author profiling is income prediction. A potential use for these estimations of income is in customer relations. Just as the rest of our life, our contact with companies and their customer services moves online. People want to be helped quickly, without giving a lot of information in advance. Customers expect companies to "know" them. Likewise, companies want to know their customers so they don't have to ask for a lot of information and can help quickly. Income can be a key factor in this, because it can help in predicting which products or services a customer probably already has and in which they are likely interested.

An example: a cable company customer with a relatively low income probably has a low tier service package. He or she will only be interested in cheap(er) options or options that provide significant extra value in comparison to their additional cost. Knowing this, offering an expensive movie channel package to this customer does not make a lot of sense. Such a package is nice to have, but isn't of much value - unless the customer is a true movie addict.


Despite the existing research into English speaking Twitter users and their respective incomes, there is currently no comparable research for Dutch speaking Twitter users. We will therefore focus our research on the following question: to what extent is it possible to accurately predict the income level of a Dutch Twitter user and which stylometric features are predictive?

The rest of this thesis is organized as follows. Chapter 2 gives an overview of earlier work on income classification with English speaking users, as well as all prerequisites for studying income classification. Chapter 3 details our data gathering efforts and goes into detail about our labeling algorithm. Chapter 4 builds on that, with our method to find the best combination of features for a classifier on income. In chapter 5, we will discuss the results of the different feature setups tested. Finally, in chapter 6 we will look back at the study as a whole and answer our research question.

\chapter{Background}
Research into author profiling on Twitter is relatively new, especially in the field of income prediction. All prior work has focused on English-speaking Twitter users.

The most recent study was performed by \citet{flekova}. Their goal was to find a viable writing style-based predictor for age and income. For each dimension, a different data set was used. 
Regarding income, an off the shelf data set was used, which will be discussed in more detail later on. It contained almost six thousand Twitter users labeled with their occupation. The researchers labeled each user with the mean UK yearly income associated with their occupation. This was done regardless of the location of the users involved (e.g., an English-speaking user from Amsterdam would also be assigned a UK income). 
As the exact income for every occupation was known, the machine learning task was framed as regression. Flekova et al. codified stylistic variation into a number of features, which were grouped into four categories: surface, readability, syntax and style. 
After performing a ten-fold cross validation with both linear and non-linear regression methods, they discovered that readability metrics like the \textit{Flesch Reading Ease} metric and the relative use of pronouns correlated with income over age. They concluded that the differences in style can be used to "tailor the style of a document without altering the topic to suite either age or income individually".

The data set used in \citet{flekova} was created during an earlier study by \citet{pietro}. They used the corpus to classify users according to their occupational class. 
The occupational titles and classes used were gathered from the UK Standard Occupational Classification (SOC) \citep{uksoc}. The SOC is a hierarchical classification of occupations. It consists of four levels, starting with nine very general classes and terminating in hundreds of very specific classes. Each level is indicated with a different number of digits. The coarsest level is indicated with one digit and the finest level with four digitals (e.g., class 1: 'managers, directors and senior officials' and class 1116: 'elected officials and representatives', respectively). The classification is based on the International Standard Classification of Occupations \citep{isco}.
For each occupation the \textit{Twitter REST API}\footnote{https://dev.twitter.com/rest/public} was used to find at most 200 users for each occupation. The accumulated users were divided into the three-digit groups that they belong to. Users that were companies, had no description or had a contradicting description, were removed from the collection by hand. Furthermore, three-digit groups with less than 45 users were discarded. The final collection contained 5191 users, divided into 55 three-digit groups.

\citet{pietro} mention two papers in their related work section that describe different labeling strategies. Both do not annotate users manually. They all employ \textit{distant supervision}, a method that labels data automatically, given an existing form of ground truth. This approach was first used by \citet{distantsupervision} for sentiment classification of tweets. In the Preotiuc-Pietro study, Twitter users were labeled by looking for a self-disclosed occupational title. In order to detect such titles they relied on a list of occupational titles from the SOC. These list forms were used by the ground truth that is needed for labeling.

The first is a paper by \citet{li} which tries to label users with the name of their employer, amongst other things. In this study, Twitter profiles were cross-linked with profiles on a different social networking site, Google Plus. To ensure both profiles are interrelated, they looked at their friend connections on both sites and made sure that there was a large enough intersection between them. The employer name was extracted from the Google Plus profile and used to label the Twitter user. Using this method, they were able to collect 7208 users with a known employer. 
This strategy relies heavily on profiles from networking sites other than Twitter, like Google Plus or LinkedIn. Without unfettered access or a large number of cross-linkable profiles on both platforms, it cannot be used for labeling data.

The second strategy relies solely on the occupational titles from the SOC and profiles on Twitter. \citet{sloan} labeled users with class in the National Statistics - Socio-Economic Classification (NS-SEC), a classification closely related and interoperable with the SOC. 
They extracted users from a feed provided by the \textit{Twitter REST API}, which constitutes a one percent representative sample of public tweets. For each user, they looked for a self-disclosed occupational title in the biography line. The titles they used for detection were gathered from the SOC. After finding an occupation, the user would be labeled with their NS-SEC class by looking up the SOC class for their occupation and consequently looking up the corresponding NS-SEC class. In case a user mentioned multiple occupations, the authors hypothesized that the first one found would be most important and therefore should be used. 
Using this method they were able to label 32,032 users with a NS-SEC class. A random survey of 1000 users resulted in an accuracy of 57.8 percent. The authors mention several caveats of this method. Hobbies and former occupations may be falsely detected as current occupations. Commonly occurring phrases like "Doctor Who fan" may also result in false matches.

This second strategy depends a lot less on outside data. It is therefore easier to implement and use in further research, given that the needed data is available.  For this thesis, a corpus of Dutch tweets and their authors is already available at the University of Groningen and data on occupational titles and incomes is available from the Dutch government bureau Statistics Netherlands\footnote{https://www.cbs.nl/en-gb}. It therefore makes sense to use this second strategy for our data collection and annotation efforts. The implementation details of this strategy follow in the next chapter.

\chapter{Data and Material}
\label{datagathering}
\section{Collection} 
The primary data set for this research is a corpus of Dutch Twitter users with their 500 latest tweets, categorized by income class. As there was no suitable data set available off the shelf, a new corpus was created. As a starting point, the University of Groningen (UG) \textit{twitter2} corpus was used to gather user profiles. The twitter2 corpus contains all Dutch tweets provided by Twitter's \textit{Streaming API}, which constitutes a one percent representative sample of public messages posted on Twitter. 

In order to gather user profiles, all tweets from September 1st till September 5th, 2016 were used. For each tweet in the corpus, the user was looked up using the UG's in-house \textit{tweet2tab} tool to extract the user ID, username, real name and biography line for each user from the corpus. The user's biography line was used to find an occupational title. That title was then linked to an occupational class and consequently the average hourly income for that occupational class. The average hourly income was then multiplied by the number of hours worked by the average Dutch worker per year, to compute the average yearly income. All users with a known occupation were labeled with their average yearly income. This resulted in a collection of 36,113 users with suggested occupations and incomes.

After removing no longer existing accounts, private accounts and accounts with less than 1000 tweets, 21,862 users were still available. These users were divided into two income classes, high (above \euro 34,500) and low (below \euro 34,500). Afterwards, 1500 users were randomly selected from each group and their tweets were gathered using the Twitter REST API. Retweets and non-Dutch tweets (as explained in the next section) were left out of the collection. Users with less than 500 Dutch, self-written tweets were discarded.\footnote{The tweets were collected on December 10, 2016 between 00:44 and 9:30.} From the remaining users, a thousand were randomly selected per class for further use in this research. More details about processing users for use in the collection will follow in the section on \textit{processing}.

\section{Annotation}
In order to divide the users into income classes, they need to be annotated with their average yearly income. Distant supervision is used to find the average yearly income of a user. We look for an occupational title of a user in the user's biography line. In case a user has multiple occupations, we use the first one we can find. With the found title, we look up the user's occupational class and the average hourly income for that class. We then label the user with the average yearly income by multiplying the average hourly income with the average total hours worked in The Netherlands. 

There are three additional data sources needed in order to make our annotation process work. 

First, we use a list of occupational titles and their respective classes from \citet{codelijsten} to look up the occupation of a user. These classes correspond with classes in the International Standard Classification of Occupations \citep{isco}, just like the earlier mentioned SOC. As this file was never meant for machinal consumption, the file had to be modified. All titles formed one long string, which had to be split in order to get the individual titles per class. Furthermore, the titles contained a lot of shorthand notations, e.g. "assistent-, coach" for the similar occupations "coach" and "assistent-coach" and slashes for synonyms like "typist / tekstverwerker". These were removed by hand as there was no suitable way to do this correctly in an automated manner. 
Second, we use a list of occupational classes and their respective average incomes from \citet{uurlonen} to look up the average hourly income for a particular class. We use the two-digit classes, as the incomes corresponding to almost all of them are known\footnote{There were no average incomes available for the two-digit (submajor) groups 62, 82, 92 and 95. They are therefore left out of the rest of our research.}. For most three- and four-digit classes, incomes aren't provided by Statistics Netherlands. Furthermore, the incomes amongst the two-digit groups vary enough to create a viable two-class split of our data.
Finally, to derive the average yearly income we need to know the average worked hours per year in The Netherlands. According to the \citet{hours} the average Dutch worker makes 1677 hours a year.

To evaluate the performance of our distant supervision method, a random survey of 100 users per class was taken. Their labels were manually checked in a two class setting, as mentioned in the previous section. The labels were considered correct if they appeared in the biography of a user, the user was a human and the occupational title was used to indicate a paying occupation, not a hobby or study. The accuracy over the whole group of 200 users was 74.5 percent, with 70 percent in the low class and 79 percent in the high class. In 17 cases, the labels were wrong because the account was simply not used by a person but by a company. As there is no surefire way to distinguish between human and non-human users, we disregard these cases. The overall accuracy without these cases is 81.4 percent. The last few cases that were wrongly labeled consist of the following cases. In 16 cases, hobbies or voluntary work were labelled as an occupation. In 14 cases, occupational titles got bodged during the transformation of the needed file and therefore detected the wrong occupation. Finally, in four miscellaneous cases users described an internship or former occupation in a non-trivially detectable way.

These results confirm that our distant supervision method yields enough correctly labeled data for our research, even though it is far from perfect. Possible future improvements of the method will follow in the \textit{Future Work} section of \autoref{conclusion}.

\section{Processing}
\label{sec:processing}
Concerning the users, after extracting users with a known occupation from the twitter2 corpus, a number of processing steps are involved to bring the entire group of 36113 users to a manageable number of suitable users. First, users that no longer exist, have their profile set as private or have less than 1000 tweets are removed, to ensure we can get enough data per user for our research. The users are checked by using the Lookup API of Twitter, which allows us to check users in batches of 100. The remaining users are saved in a Python dictionary and written to disk with the built-in Pickle library. Afterwards, the remaining users are divided in the chosen income classes. From each class 2000 users are randomly selected for further processing by using the random.choice function of the NumPy Python library \citep{numpy}. For these users, their latest 1000 tweets are collected. Retweets and non-Dutch tweets were discarded. The language classification of each tweet was performed by the langid Python library \citep{langid}. Users with less than 500 suitable tweets were left out of the data set. For all remaining users, the tweets are written to a text file per user per class. From the remaining set of users, 1000 were randomly selected per group to be used in this research.

After the processing of the users is completed, the tweets are prepared for further use. URLs, hashtags and usernames are removed and the tweets are tokenized, so that relevant features can be derived from the tweets. The processing relies on the \textit{TweetTokenizer} included in the NLTK Python library \citep{nltk}, a popular library for natural language processing in Python. As some features need to be generated from text tokenized per sentence, all tweets were run through a pre-trained NLTK sentence tokenizer created by \citet{punkt}. It was trained on the Dutch part of the Multilingual Corpus 1 (ECI), particularly on articles from the "De Limburger" newspaper. The resulting data was collected in a dictionary and written to disk with the Pickle library.


\chapter{Method}
\citet{flekova} was a major source of inspiration for our methodology. However, as our research is limited in available time (7 weeks) and resources, we had to make some compromises, given all constraints. We mainly rely on the scikit-learn library \citep{sklearn} for the concrete implementation of our method. It provides a lot of tools for machine learning, from full blown classifiers and regression models to tools for evaluation. 

\section{Features}
Originally, we planned to implement all of the feature groups used in \citet{flekova}. Due to the constraints of our research, we could only adopt two of the four groups: surface and readability. To compensate for this, we added a third groups of features: n-grams. All groups will be discussed in more detail below.

\subsection{Surface}
From this group, we chose and implemented the following features:
\begin{itemize}
\item Length of a user's tweets in words;
\item Length of a user's tweets in characters;
\item Average word length in a user's tweets;
\item Ratio of words longer than 5 characters in a user's tweets;
\item Type-token ratio.
\end{itemize}

As these features mostly rely on counting words and performing some basic calculations, they were relatively easy to implement. They are all based on tokenized tweets without punctuation. The last two features partly overlap with the group of readability features, as they are considered predictors of readability \citep{flekova}.

\subsection{Readability}
\citet{flekova} used a host of readability measures. They all have some commonality in the way they are calculated, but differ in measuring scale and intended application. Instead of implementing each measure with all its peculiarities by hand, we relied on the \textit{readability} library \citep{readability} for their calculation. This library provides a function that takes sentence tokenized text as its input and outputs the scores for several readability metrics. As sentence tokenization has already been performed by NLTK, we only need to provide the right input data for each user to calculate its scores. These texts do include punctuation symbols, as they are needed for the calculation of some measures.

The readability metrics included in our research are:
\begin{itemize}
\item Automated Readability Index;
\item Coleman-Liau Index;
\item Flesch-Kincaid Grade Level;
\item Flesch Reading Ease;
\item Gunning-Fog Index;
\item LIX Index;
\item SMOG Index.
\end{itemize}

\subsection{N-grams}
For the final feature set, we looked at the syntax and style sets in the study by \citet{flekova}. 
The syntax set proved unimplementable in this short timeframe. Although the Alpino parser \citep{alpino} could provide us with the needed part-of-speech tags, the parser could not do so reliably. The parser would sometimes crash on input it could not handle.

We therefore moved on to the style set. Whilst some features like the ratio of words with numbers were easily implementable, others would prove more problematic. A lot of features depend on part-of-speech tags. Without a proper functioning parser we cannot implement those features in this research. 
We had no other choice than looking elsewhere for a possible feature set to add.

Inspired by the work of fellow student Reinard van Dalen on classifying users based on their political preference, we decided to opt for word n-grams as our third and final feature set. We use a modified version of a function provided by our supervisor in order to generate the n-grams. It calls on NLTK's \textit{ngrams} function to compute all possible n-grams for a given text.

We chose to include unigrams, bigrams and trigrams in our research. They are counted in a binary fashion: the fact that a certain n-gram occurs in a user's text is recorded, but the amount of occurrences is not taken into account.

\section{Classification}
As stated in the previous chapter, incomes are not available for every occupation, but only for a select number of occupational classes. The machine learning task ahead of us should therefore be framed as classification, instead of regression.
Handling in the spirit of \citet{flekova}, we tried to find a classification method that is closest to the methods they have used, despite the difference in prediction task. They used linear regression and vector support regression with an RBF kernel as its non-linear counterpart for comparison. 

Two classification methods came to mind: a support vector machine (SVM) and logistic regression. We chose to only use the latter. Scikit-learn's documentation on SVM's states that "if the number of features is much greater than the number of samples, the method is likely to give poor performance" \citep{svm}.
As one of our feature sets is n-grams, one can expect thousands of features, while the data set only consists of two thousand samples. We therefore shifted our focus to logistic regression and implemented it in our classifier using scikit-learn's \textit{LogisticRegression} module.

\section{Evaluation}
After implementing all features, the performance of the classifier and its composing features is assessed by using $k$-fold cross validation. In our case we apply a $k$ of 10, like \citet{flekova}.

For each fold, the results were printed to screen, including a list of most informative features. After each validation run the precision, recall and F1-scores are calculated in order to compare the different classifier setups. We test a host of different setups in order to find the most effective combination (i.e. giving us the highest F1-score with the least amount of features).  The tested feature compositions and their results are shown in  \autoref{results-overview}.

The classifiers will all be compared to a baseline. As the data set consists of two equally large groups, a majority baseline cannot be used. We therefore use a random baseline. Given that there are two classes and they are equal in size, the chance that a single instance belongs to a certain class is 50 percent ($Pcorrect = 0.5$). We rely on scikit-learn's \textit{StratifiedKFold} module to split our data for each of the ten folds. The stratification makes sure that the equality in class size in our data set will be carried through to the individual test and training sets, and thus guarantees that our baseline will be the same on each run.

\chapter{Results and Discussion}
\begin{table}[]
\caption{An overview of precision, recall and F1-scores for the different classifier setups.}
\label{results-overview}
\begin{tabular}{@{}lllll@{}}
\toprule
\textit{\#} & \textbf{Setup}                & \textbf{Precision (P)} & \textbf{Recall (R)} &\textbf{F1-score}   \\ \midrule
1  & Baseline                                  & 0.50           & 0.50        & 0.50 \\
2  & Surface                                   & 0.56          & 0.56       & 0.56 \\
3  & Readability                               & 0.57          & 0.57       & 0.57 \\
4  & N-grams (n=1)                             & 0.70          & 0.70       & 0.70 \\
5  & N-grams (n=2)                             & 0.69          & 0.69       & 0.69 \\
6  & N-grams (n=3)                             & 0.68          & 0.68       & 0.68 \\
7  & N-grams (n=1-2)                           & 0.72          & 0.72       & 0.72 \\
8  & N-grams (n=1-3)                           & 0.71          & 0.71       & 0.71 \\
9  & N-grams (n=2-3)                           & 0.70          & 0.70       & 0.70 \\
10 & N-grams (n=1-2-3)                         & 0.72          & 0.72       & 0.72 \\
11 & Surface + Readability                     & 0.59          & 0.59       & 0.59 \\
12 & Surface + N-grams (n=1)                   & 0.70          & 0.70       & 0.70 \\
13 & Surface + N-grams (n=2)                   & 0.70          & 0.70       & 0.70 \\
14 & Surface + N-grams (n=3)                   & 0.67          & 0.67       & 0.67 \\
15 & Readability + N-grams (n=1)               & 0.70          & 0.70       & 0.70 \\
16 & Readability + N-grams (n=2)               & 0.69          & 0.69       & 0.69 \\
17 & Readability + N-grams (n=3)               & 0.68          & 0.68       & 0.68 \\
18 & Surface + N-grams (n=1-2)                 & 0.71          & 0.71       & 0.71 \\
19 & Surface + N-grams (n=1-3)                 & 0.70          & 0.70       & 0.70 \\
20 & Surface + N-grams (n=2-3)                 & 0.70          & 0.70       & 0.70 \\
21 & Surface + N-grams (n=1-2-3)               & 0.71          & 0.71       & 0.71 \\
22 & Readability + N-grams (n=1-2)             & 0.72          & 0.72       & 0.72 \\
23 & Readability + N-grams (n=1-3)             & 0.71          & 0.71       & 0.71 \\
24 & Readability + N-grams (n=2-3)             & 0.70          & 0.70       & 0.70 \\
25 & Readability + N-grams (n=1-2-3)           & 0.72          & 0.72       & 0.72 \\
26 & Surface + Readability + N-grams (n=1)     & 0.69          & 0.69       & 0.69 \\
27 & Surface + Readability + N-grams (n=2)     & 0.70          & 0.70       & 0.70 \\
28 & Surface + Readability + N-grams (n=3)     & 0.66          & 0.66       & 0.66 \\
29 & Surface + Readability + N-grams (n=1-2)   & 0.71          & 0.71       & 0.71 \\
30 & Surface + Readability + N-grams (n=1-3)   & 0.70          & 0.70       & 0.70 \\
31 & Surface + Readability + N-grams (n=2-3)   & 0.69          & 0.69       & 0.69 \\
32 & Surface + Readability + N-grams (n=1-2-3) & 0.72          & 0.72       & 0.72 \\ \bottomrule
\end{tabular}
\end{table}

All evaluation results for the different classifier setups and our baseline method can be found in \autoref{results-overview}. All setups outperformed our baseline, but the extent differs quite a lot. 

Overall, it is clear that the n-gram feature groups are by far performing the best. Surface and readability feature groups, either alone or combined, reach an F1-score in the higher fifties, whereas the n-gram feature groups score at least in the higher sixties. It seems that the limited scope of features, provided by the surface and readability sets, is extensively outweighed by the n-gram sets. 

Now, it has to be determined which classifier is the most robust. There are five setups that showed equal performance, all reaching an F1-score of 0.72. These setups are:
\begin{itemize}
\item \#7: N-grams (n=1-2).
\item \#10: N-grams (n=1-2-3);
\item \#18: Readability + N-grams (n=1-2);
\item \#25: Readability + N-grams (n=1-2-3);
\item \#32: Surface + Readability + N-grams (n=1-2-3);
\end{itemize}

In order to draw conclusions on robustness, the standard deviations of the intermediate F1-scores, provided by the individual folds of the 10-fold cross validation, are calculated. The calculation was performed by IBM SPSS, a popular statistical software package. The outcomes can be found in \autoref{standarddev}.

Although all setups yield roughly equal minimum and maximum F1-scores, there is some difference in standard deviation. An interesting finding is the negative influence of adding readability or both surface and readability features to the n-gram features. In all cases considered, the n-gram setups have a lower standard deviation than comparable setups with additional features.

The setup with uni-, bi- and trigrams clearly stands out as the most robust method, having a standard deviation of 0.01581. The results make clear that simply adding more and more features does not necessarily have a positive effect on a method's robustness. We can see that the standard deviation drops when we add trigram features to a setup with uni- and trigram features. This improvement is diminished when we add even more features, like readability or both surface and readability features. 


\begin{table}[]
\caption{An overview of the minimum and maximum F1-scores as well as the standard deviation of each selected setup.}
\label{standarddev}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{\#} & \textbf{Minimum F1} & \textbf{Maximum F1} & \textbf{Average F1} & \textbf{Standard deviation} \\ \midrule
7                    & 0.68             & 0.75             & 0.72             & 0.02221                     \\
10                   & 0.69             & 0.74             & 0.72             & 0.01581                     \\
18                   & 0.68             & 0.75             & 0.72             & 0.02321                    \\
25                   & 0.69             & 0.74             & 0.72             & 0.01767                     \\
32                   & 0.68             & 0.75             & 0.72             & 0.02119                     \\ \bottomrule
\end{tabular}
\end{table}

\chapter{Conclusion}
\label{conclusion}
\section{Summary}
We started our research with the question: \textit{to what extent is it possible to accurately predict the income level of a Dutch Twitter user and which stylometric features are predictive?} Earlier work on English speaking users looked promising. \citet{flekova} concluded that differences in style could be used to "tailor the style of a document without altering its topic to either income or age individually". This matched our hopes for a potential application of this technique: tailoring customer service communication to customers without having to ask them for background information.

As a comparable study has not been done yet, we had to create a dataset from scratch. By employing the labeling strategy established by \citet{sloan}, we were able to create a corpus of 21862 users with known incomes. They were divided into two income classes, one above and one below \euro 34.500. From each class thousand users were selected for further research. Their 500 latest tweets were preprocessed for use in our classifier.

In order to classify our users we employed a logistic regression method provided by scikit-learn \citep{sklearn} with three sets of features: surface, readability and n-gram features. After testing a host of different feature group combinations on our classifier, we managed to get a maximum F1-score of 0.72 using 10-fold cross validation, outperforming our baseline F1 of .5. The feature set with only uni-, bi- and trigrams proved to be the most robust, providing the highest average F1-score with the lowest standard deviation.

\section{Limitations}
As our study was limited in the time and resources available, it is far from conclusive. We will describe a few of its limitations below.

\subsection{Availability of income data}
The first important limitation of our study is the level of detail in the income data used to label our users. As Statistics Netherlands does not provide incomes for all occupations, we were forced to use and predict income classes instead of individual incomes. By using the average incomes for each two-digit group, chances are that there is a large gap between the predicted income and the actual income of a user. This might lead to erroneous classifications. As we have no full income data, we cannot assess the size of the problem at this time.

\subsection{Labeling strategy}
Our labeling strategy worked reasonably well, as demonstrated by the random sample in \autoref{datagathering}. However, it is far from perfect. One problem is that our method cannot distinguish between human and non-human users. Although this is such a complex topic that it would warrant a separate study, adding this capability would help to reduce the noise in our dataset. Another problem is that our labeling method relies on just one heuristic: it will always take the first occupation it can find, even if there are multiple occupations mentioned by a user. It currently has no way to disambiguate between multiple occupations. We will present possibilities for future research on our labeling strategy in the next section.

\subsection{Class and classifier setup}
We originally planned to use two different class setups: a two class setting with high and low classes and a six class setting used by Statistics Netherlands (\euro 0 - \euro 10.000, \euro 10.000 - \euro 20.000, \euro 20.000 - \euro 30.000, \euro 30.000 - \euro 40.000, \euro 40.000 - \euro 50.000 and \euro 50.000 or more). As our data gathering and annotation efforts proved to be quite challenging, given that there was no prior gathering and annotation method for Dutch users, we decided to limit ourselves to the two class setting.

For the same reason, we had to limit our exploration of different classifier setups. We planned to do a thorough review like \citet{flekova}, where each individual feature was measured on its predictiveness using two different regression methods. We limited ourselves to one classification method (logistic regression) as the documentation of scikit-learn indicated that \textit{support vector machines} would likely give poor performance on our feature sets \citep{svm}. This would have mainly affected setups with n-gram features. Exploration of different classification methods may prove worthwhile in improving results.

Regarding the included features, we only assessed the influence of different feature set combinations, including different levels of n-grams, as they greatly influence the amount of features available to our classifier. This limited assessment already yielded 31 distinct feature group combinations to be evaluated. As this proved to be time consuming, more detailed evaluations had to be left out.

\section{Future work}
\label{futurework}
A major boost to future work on profile Dutch authors on income would be the availability of incomes on a full four-digit level so individual incomes can be predicted and not just income classes. However, as we depend on Statistics Netherlands for our income data, it is out of our hands. For now, there are three clear avenues for future research using the available data.

\subsection{Improvement of labeling method}
Although our current labeling method is a good basis, it could become more robust. As already mentioned, disambiguation between multiple occupations by selecting the one with the highest ISCO class would be one possibility for improving our labeling accuracy, as it would also limit the amount of hobbies, studies and volunteer's work that was falsely detected as occupations in cases where multiple candidate occupations are present.

\subsection{Different class setups}
As we only tested a two class setup, it would make sense to test the performance of our classifier on a data set with more than two classes. This would also bring the income classes closer to the actual income of the user, thus resulting in more valuable predictions. It would be interesting to see how the predictive power of certain feature sets hold up in a different situation.

\subsection{In-depth analysis of features}
As we only investigated the influence of entire feature groups on our classifier's performance, it might be worthwhile to research the influence of individual features in the surface and readability groups. It might become clear that an entire different mix of individual features yields better results than the best ones our current setup could provide.

\section{Final statement}
This study has been a first exploration of the possibilities of profiling Dutch authors on their income. We hope that this may be the basis of further research and, in combination with more detailed income data, be a stepping stone towards predicting the income of individual Dutch authors. 


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{chicago} 
\bibliography{thesis-IK}


%----------------------------------------------------------------------------------------
%	APPENDICES
%----------------------------------------------------------------------------------------
\newpage
\appendix
\chapter{Appendix}
\section{List of occupational classes and respective occupational titles}
As this file is very long, it could not be included in the text. The file is provided in the zip-file included with this submission and is also provided online via: \\ \url{https://www.thesociallions.nl/thesis/}.

\newpage
\section{List of average hourly incomes for each occupation class}
\begin{table}[h]
\label{appendix:income}
\begin{tabular}{@{}ll@{}}
\toprule
Income class & Average hourly income (\euro) \\ \midrule
11 & 43.02 \\
12 & 39.95 \\
13 & 35.77 \\
14 & 22.86 \\
21 & 29.10 \\
22 & 30.91 \\
23 & 28.40 \\
24 & 31.50 \\
25 & 28.45 \\
26 & 28.30 \\
31 & 26.33 \\
32 & 21.72 \\
33 & 25.42 \\
34 & 20.42 \\
35 & 24.80 \\
41 & 19.42 \\
42 & 16.96 \\
43 & 22.24 \\
44 & 18.90 \\
51 & 15.74 \\
52 & 16.24 \\
53 & 19.12 \\
54 & 20.65 \\
61 & 16.12 \\
71 & 20.60 \\
72 & 19.17 \\
73 & 19.07 \\
74 & 19.45 \\
75 & 17.73 \\
81 & 17.71 \\
83 & 17.52 \\
91 & 13.89 \\
93 & 15.28 \\
94 & 13.23 \\
96 & 17.03 \\ \bottomrule
\end{tabular}
\end{table}

\newpage
\section{List of commands for using our toolset}
The toolset is divided into three modules: \textit{datagathering}, \textit{preprocessing} and \text{machinelearning}. All included scripts can be imported as a module in a Python script and be used as a user wishes. Full docstrings are provided in the files themselves. The libraries used can be installed by using the included requirements.txt file, using the command \textit{pip install -r requirements.txt}. The commands below list all scripts that can run standalone in order to recreate our setup.

By default, all created files are written to the \textit{output\_files} folder in the \textit{supportdata} directory of the project. Reference output files are provided in the folder \textit{reference\_output} in the same directory.

\subsection{\textbf{Data gathering}}
All scripts for data gathering can be found in the \textit{datagathering} folder of the project. The description below will guide you through all needed steps to gather the labeled user data.

\subsubsection{Loading income data from a CSV file}
\textbf{Script}:	import\_incomes.py \\
\textbf{Default command} python3 import\_incomes.py <path\_to\_input\_file>\\
\textbf{Input}: a comma separated file with submajor groups in the first column and their respective average hourly incomes in the second column. The file used in my research is \textit{hourly\_income\_per\_submayor\_group.csv}, which is situated in \textit{supportdata/input\_data/}. \\
\textbf{Output}: a Python dictionary containing the submajor groups as keys and incomes as values, written to disk with Pickle. \\
\textbf{Comments}: the method uses \textit{; } as delimiter by default, as it is the one used by Microsoft Excel. 

\subsubsection{Connecting occupational titles with the average income for their submajor group}
\textbf{Script}:	connect\_titles\_with\_incomes.py \\
\textbf{Default command}: python3 connect\_titles\_with\_incomes.py <path\_to\_input\_file>\\
\textbf{Input}: a comma separated file with submajor groups in the first column and a comma separated string of example occupational titles in the third column. The file used in my research is \textit{edited\_occupational\_titles.csv}, which is situated in  \textit{supportdata/input\_data/}. \\
\textbf{Output}: a Python dictionary containing the example occupational titles as keys and a Tuple containing occupational class and income as values, written to disk with Pickle. \\
\textbf{Comments}: the method uses \textit{; } as delimiter by default, as it is the one used by Microsoft Excel. 

\subsubsection{Collecting users with known incomes from the \textit{twitter2} corpus}
\textbf{Script}:	collect\_labeled\_users.py \\
\textbf{Default command}: zcat /net/corpora/twitter2/Tweets/2016/09/2016090{1..5}* | tweet2tab user.id user user.name user.description | python3 collect\_labelled\_users.py\\
\textbf{Input}: a tab separated user data from the twitter2 corpus, generated with tweet2tab as demonstrated in the above command. We extract the user id, username, full name and description for each user. This data is loaded by default via standard input, but can also be read from a similarly formatted file by calling the script with a parameter containing the input file as first argument. \\
\textbf{Output}: a Python dictionary containing the user id's as keys and a Tuple containing username, real name,
     description, found occupation, occupational class and incomes, written to disk with Pickle.\\
\textbf{Comments}: this script requires output from the \textit{twitter2} corpus formatted with \textit{tweet2tab} or a similar corpus that provided similarly formatted output for use as input data. The \textit{twitter2} corpus can be found on the \textit{Karora} server of the University of Groningen. 

\subsubsection{Removing non-existing users, private users and user with less than thousand tweets}
\textbf{Script}:	check\_user\_profiles.py \\
\textbf{Default command}: python3 check\_user\_profiles.py \\
\textbf{Input}: a Pickle file containing a dictionary with user id's as keys and Tuples with user information as values. This file is generated by the previous script. The file used in my research is \textit{labeled\_users.pickle}, which is situated in  \textit{supportdata/input\_data/}.  \\
\textbf{Output}:  a Python dictionary containing the user id's as keys and a Tuple containing the user data, written to disk with Pickle. \\
\textbf{Comments}: to use the Twitter API, we need a set of access tokens. These can be requested at \url{https://dev.twitter.com}. We used six sets of access tokens to speed up our data gathering. The tokens need to be entered in a JSON file named credentials.json, located in the directory \textit{datagathering}/\textit{twitterapi}/\textit{private}. An example file is provided in the same folder.

\subsubsection{Creating two class split of users}
\textbf{Script}:	create\_income\_classes.py \\
\textbf{Default command}: python3 create\_income\_classes.py\\
\textbf{Input}:  a Pickle file containing a dictionary with user id's as keys and Tuples with user information as values, generated by the previous script. The file used in my research is \textit{suitable\_users.pickle}, which is situated in \textit{supportdata/output\_files}. \\
\textbf{Output}:  a Python dictionary containing the class labels as keys and Lists containing a List per user with its
    user id as Int and user information in a Tuple, written to disk with Pickle.\\
\textbf{Comments}: by default, the users are split in a low and high class, respectively with an average yearly income below \euro 34.500 and above.

\subsubsection{Manual evaluation of labels gathered with distant supervision}
\textbf{Script}:	evaluate\_annotations.py \\
\textbf{Default command}: python3 evaluate\_annotations.py\\
\textbf{Input}:  a Pickle file, containing a dictionary with the class labels as keys and Lists containing a List per user with its
    user id as Int and user information in a Tuple, as generated by the previous script. The file used in my research is \textit{users\_in\_classes.pickle}, which is situated in the \textit{output\_data} folder of \textit{supportdata}. \\
\textbf{Output}:  the outcomes of the manual evaluation are only printed to screen. \\
\textbf{Comments}: by default, the script picks 100 users per class. 

\subsubsection{Preselection of user for retrieval of tweets}
\textbf{Script}:	pre\_select\_users.py \\
\textbf{Default command}: python3 pre\_select\_users.py\\
\textbf{Input}:  a Pickle file, containing a dictionary with the class labels as keys and Lists containing a List per user with its
    user id as Int and user information in a Tuple, as generated by the previous script. The file used in my research is \textit{users\_in\_classes.pickle}, which is situated in the \textit{output\_data} folder of \textit{supportdata}. \\
\textbf{Output}:  a Python dictionary containing the classes as keys and a List of selected user id's as values, written to disk with Pickle. \\
\textbf{Comments}: by default, the script picks 2000 users per class. 

\subsubsection{Gathering of user posts}
\textbf{Script}:	gather\_user\_posts.py \\
\textbf{Default command}: python3 gather\_user\_posts.py \\
\textbf{Input}:  a Pickle file, containing a dictionary with the class labels as keys and Lists with the selected user id's as values, as generated by the previous script. The file used in my research is \textit{preselected\_users.pickle}, which is situated in the \textit{output\_data} folder of \textit{supportdata}. \\
\textbf{Output}:  a corpus of user's tweets, written to text files. Every user has a separate text file with its id in the folder of its class. \\
\textbf{Comments}: by default, the script loads the latest 1600 tweets of the user. Furthermore, it writes all suitable tweets to disk.

\subsubsection{Selection of users for further research}
\textbf{Script}:	post\_select\_users.py \\
\textbf{Default command}: python3 post\_select\_users.py \\
\textbf{Input}:  a String containing the location of the gathered tweets. By default, this is the \textit{corpus} folder. \\
\textbf{Output}:  a Python dictionary containing the classes as keys and a List of selected user id's as values, written to disk with Pickle. \\
\textbf{Comments}: by default, the script picks 1000 users per class.

\subsection{\textbf{Preprocessing}}
\subsubsection{Preprocessing of users and their tweets}
\textbf{Script}:	tokenizer.py \\
\textbf{Default command}: python3 tokenizer.py \\
\textbf{Input}:  a Pickle file, containing a dictionary with the class labels as keys and Lists with the selected user id's as values, as generated by the previous script.  \\
\textbf{Output}:  a Python dictionary containing the classes as keys and a List with a Tuple per user, holding all tokens per tweet in a List and sentence tokenized text as a String, written to disk with Pickle. \\
\textbf{Comments}: punctuation is removed from the tokens, but not from the sentence tokenized text. The punctuation is needed to calculate the readability measures.

\subsection{\textbf{Machine learning}}
\subsubsection{Generating prefeaturized dictionaries for evaluation}
\textbf{Script}:	generate\_test\_dicts.py \\
\textbf{Default command}: python3 generate\_test\_dicts.py \\
\textbf{Input}:  none.  \\
\textbf{Output}:  a Python dictionaries containing the different feature setups, written to disk with Pickle. \\
\textbf{Comments}: we extracted the features before running the evaluation set, as this can be quite time consuming on runtime.

\subsubsection{Evaluation of different feature setupd}
\textbf{Script}:	evaluation.py \\
\textbf{Default command}: python3 evaluation.py \\
\textbf{Input}:  none.  \\
\textbf{Output}: the outcomes of the cross validation runs are printed to screen. \\
\textbf{Comments}: the paths to the dictionaries containing the features for every setup by the former script is hardcoded.

\end{document}



