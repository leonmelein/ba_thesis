%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
% Johan Bos (johan.bos@rug.nl)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
%BCOR5mm, % Binding correction
] {book}% {scrartcl}
\usepackage{eurosym}
\usepackage{booktabs}
\usepackage{csvsimple,longtable}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout


%----------------------------------------------------------------------------------------
%	HYPHENATION
%----------------------------------------------------------------------------------------

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether



%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{title}} % The article title

\author{\spacedlowsmallcaps{author}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

%\renewcommand{\chaptermark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
%\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\hypersetup{pageanchor=false}
\begin{titlepage}
\thispagestyle{empty}
\begin{figure}[h!] %  figure placement: here, top, bottom, or page
\includegraphics[width=4in]{ruglogo} 
\end{figure}

\begin{center}
\vspace{30 mm}
\begingroup \linespread{1,75} \selectfont 
\textsc{\LARGE Detecting Income Level of Dutch Twitter Users using Stylometric Features}\\
% \textsc{\Large And this is an optional subtitle}
[1,5cm]
\endgroup

L\'eon Melein\\[2,5cm]

\end{center}
\vfill
\textbf{Bachelor thesis - Concept version}\\  %\textbf{Master thesis}\\
Information Science\\  %Information Science\\
L\'eon Melein\\
S2580861\\
\today\\
Supervisor: B. Plank
\end{titlepage}
\hypersetup{pageanchor=true}



%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\pagenumbering{roman}
\chapter*{Abstract}
\markboth{Abstract}{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
Income prediction is a relatively undiscovered aspect of author profiling. Early research on English speaking like \citet{flekova}, who linked Twitter users  to occupations and their respective average incomes, has been promising, but there is no comparable research for Dutch speakers yet. In this thesis, we explore to what extent author profiling can predict the income level of Dutch users.

We do so by creating a dataset of 2000 Twitter users. These are divided into two income classes, as there currently is no complete income data available for individual occupations in The Netherlands, but only for groups of occupations. We use distant supervision to annotate users with their occupational class and their income. We then extract a number of surface, readability and n-gram features from the users' posts. Using logistic regression, we try to classify the users on their income class with those features.

After testing various feature groupings, the classifier was the most effective with unigram and bigram features, reaching an F1-score of 0.72. Although this indicates that  profiling can predict a user's income class to a very large extent, this can only be seen as a first indication as the scope of this study is limited. With clear directions for future improvement, we hope that this study may be a stepping stone towards the prediction of individual incomes for Dutch authors.

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------
\clearpage
\setcounter{tocdepth}{3} % Set the depth of the table of contents to show sections and subsections only
\tableofcontents % Print the table of contents

%\listoffigures % Print the list of figures (optional, only if you have many figures)

%\listoftables % Print the list of tables (optional, only if you have many tables)

%\lstlistoflistings



%----------------------------------------------------------------------------------------
%	Preface
%----------------------------------------------------------------------------------------

\chapter*{Preface}
\markboth{Preface}{Preface}
\addcontentsline{toc}{chapter}{Preface}

I would like to take this opportunity to thank a number of people who have been instrumental to finishing my thesis. 
First of all, my supervisor Barbara. Thanks to her relentless support and guidance I was able to tackle my research question. Even when my research proved more challenging than expected, she helped me in tackling the problems step by step. 

I also want to thank fellow student Reinard van Dalen for his advice and support. As our thesis subjects partly overlap, it really helped to look at certain problems together and then solve them in the most efficient way possible. And a final thank you to Stijn Eikelboom, who proofread my thesis.


Their support could obviously not prevent the toll my thesis took on my keyboard. It broke down while I was writing the first chapters. It turned out to be a worthwhile sacrifice for completing my thesis. 




%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\chapter{Introduction}
\pagenumbering{arabic}
As we rely more and more on the internet and its applications in our daily lives, the opportunities for author profiling continue to grow. Social media services, and the text-based communications they facilitate, provide an ever-growing corpus of texts, which are linked to their author. Furthermore, these services provide additional self-disclosed information about the authors like education, occupation and relationships. If we use this data the right way, we can perform research that simply wasn't possible before or on a much bigger scale than was possible before \citep{sloan}.


An aspect of author profiling is income prediction. A potential use for these estimations of income is in customer relations. Just as the rest of our life, our contact with companies and their customer services moves online. People want to be helped quickly and without giving a lot of information in advance. They expect companies to "know" them. Companies want to know their customers so they don't have to ask for a lot of information and can help quickly. Income can be a key factor in this, because it can help in determining which products or services a customer probably already has and in which it is likely interested.

An example: a cable company customer with a relatively low income probably has a low tier service package. He or she will only be interested in cheap(er) options or options that provide significant extra value, in comparison to their additional cost. Knowing this, offering an expensive movie channel package to this customer does not make a lot of sense. Such a package is nice to have, but isn't of much value - unless the customer is a true movie addict.


Despite the existing research into English speaking Twitter users and their respective incomes, there is currently no comparable research for Dutch speaking Twitter users. We will therefore focus our research on the following question: to what extent is it possible to accurately predict the income level of a Dutch Twitter user and which stylometric features are predictive?

The rest of this thesis is organized as follows. Chapter 2 gives on overview of earlier work on income classification with English speaking users, as well as all prerequisites for studying income classification. Chapter 3 details our data gathering efforts and goes into detail about our labeling algorithm. Chapter 4 builds on that with our method to find the best combination of features for a classifier on income. In chapter 5, we will discuss the results of the different feature setups tested. Finally, in chapter 6 we will look back at the study as a whole and answer our research question.

\chapter{Background}
Research into author profiling on Twitter is relatively new, especially in the field of income prediction. All prior work has focused on English-speaking Twitter users.

The most recent study was performed by \citet{flekova}. Their goal was to find a viable writing style-based predictor for age and income. For each dimension, a different data set was used. 
Regarding income, an off the self data set was used, which will be discussed in more detail later on. It contained almost six thousand Twitter users labeled with their occupation. The researchers used the occupations to label the users with the mean UK yearly income for their occupation. This was done regardless of the location of the users involved (e.g., an English-speaking user from Amsterdam would also be assigned a UK income). 
As the exact income for every occupation was known, the machine learning task was framed as regression. Flekova et al. codified stylistic variation into a number of features, which were grouped into four categories: surface, readability, syntax and style. 
After performing a ten-fold cross validation with both linear and non-linear regression methods they discovered that readability metrics like the \textit{Flesch Reading Ease} metric and the relative use of pronouns correlated with income over age (\(r_{income} = .297,  r_{Flesch} = .315;  p < 0.001 \) ). They concluded that the differences in style can be used to "tailor the style of a document without altering the topic to suite either age or income individually".

The data set used in \citet{flekova} was created during an earlier study by \citet{pietro}. They used the corpus to classify users according to their occupational class. 
The occupational titles and classes used were gathered from the UK Standard Occupational Classification (SOC) \citep{uksoc}. The SOC is a hierarchical classification of occupations. It has four levels, starting with nine very general classes and terminating in hundreds of very specific classes. Each level is indicated with a different number of digits. The coarsest level is indicated with one digit and the finest level with four digitals (e.g., class 1: 'managers, directors and senior officials' and class 1116: 'elected officials and representatives', respectively). The classification is based on the International Standard Classification of Occupations \citep{isco}.
For each occupation they used the Twitter REST API to find at most 200 users for each occupation. The accumulated users were divided into the three-digit groups they belong to. Users that were companies, had no description or had a contradicting description were removed from the collection by hand. Furthermore, three-digit groups with less than 45 users were discarded. The final collection contained 5191 users divided into 55 three-digit groups.

Preotiuc-Pietro et al. mention two papers in their related work section that describe different labeling strategies. Both do not annotate users manually. They all employ \textit{distant supervision}, a method that labels data automatically, given an existing form of ground truth. In the Preotiuc-Pietro study, Twitter users were labeled by looking for a self-disclosed occupational title. In order to detect such titles they relied on a list of occupational titles from the SOC. This list forms the ground truth needed for labeling.

The first is a paper by \citet{li} which tries to label users with the name of their employer, among other things. In this study, Twitter profiles were cross-linked with profiles on a different social networking site, Google Plus. To ensure the both profiles are interrelated they looked at the friend connections on both sites and made sure that there was a large enough intersection between them. The employer name was extracted from the Google Plus profile and used to label the Twitter user. Using this method they were able to collect 7208 users with a known employer. 
This strategy relies heavily on profiles from networking sites other than Twitter, like Google Plus or LinkedIn. Without unfettered access or a large number of cross-linkable profiles on both platforms, it cannot be used for labeling data.

The second strategy only relies solely on the occupational titles from the SOC and profiles on Twitter. \citet{sloan} labeled users with class in the National Statistics - Socio-Economic Classification (NS-SEC), a classification closely related and interoperable with the SOC. 
They extracted users from a feed provided by the Twitter API, which constitutes a 1 percent representative sample of public tweets. For each user, they looked for an self-disclosed occupational title in the biography line. The titles they used for detection were gathered from the SOC. After finding an occupation, the user would be labeled with their NS-SEC class by looking up the SOC class for their occupation and then looking up the corresponding NS-SEC class. In case a user mentioned multiple occupations, the authors hypothesized that the first one found would be most important and therefore should be used. 
Using this method they were able to label 32032 users with their NS-SEC class. A random survey of 1000 users resulted in an accuracy of 57,8 percent. The authors mention several caveats of this method. Hobbies and former occupations may be falsely detected as current occupations. Commonly occurring phrases like "Doctor Who fan" may also result in false matches.

This second strategy depends a lot less on outside data. It is therefore easier to implement and use in further research, given that the needed data is available.  For this thesis, a corpus of Dutch tweets and their authors available is already at the University of Groningen and data on occupational titles and incomes is available from the Dutch government bureau Statistics Netherlands. It therefore makes sense to use this strategy for our data collection and annotation efforts. The implementation details of this strategy follow in the next chapter.

% TODO: FIND OUT HOW TO REPORT STATICTICAL FINDINGS THE RIGHT WAY

\chapter{Data and Material}
\label{datagathering}
\section{Collection} 
The primary data set for this research is a corpus of Dutch Twitter users with their 500 latest tweets, categorized on income class. As there was no suitable data set available off the shelf, a new corpus was created. As a starting point, the University of Groningen (UG) twitter2 corpus was used to gather user profiles. The twitter2 corpus contains all Dutch tweets provided by Twitter's Streaming API, which constitutes a 1 percent representative sample of public messages posted on Twitter. 

In order to gather user profiles, we used all tweets from september 1th till september 5th, 2016. For each tweet in the corpus, we looked up its user by using the UG's in-house \textit{tweet2tab} tool to extract the user ID, username, real name and biography line for each user from the corpus. The user's biography line was used to find an occupational title. That title was then linked to an occupational class and consequently the average hourly income for that occupational class. The average hourly income was then multiplied by the average number of worked hours in the Netherlands to compute the average yearly income. All users with a known occupation were labeled with their average yearly income. This resulted in a collection of 36113 users with known occupations and incomes.

After removing no longer existing accounts, private accounts and accounts with less than 1000 tweets, 21862 users were still available. These users were divided into two income classes, high (above \euro 34.500) and low (below \euro 34.500). Afterwards, 1500 users were randomly selected from each group and their tweets were gathered using the Twitter API. Retweets and non-Dutch tweets (as explained in the next section) were left out of the collection. Users with less than 500 Dutch, self-written tweets were discarded. From the remaining users, 1000 users were randomly selected per class for further use in our research. More details about the processing of the users for use in the collection will follow in the section on \textit{processing}.

\section{Annotation}
In order to divide the users into income classes, they need to be annotated with their average yearly income. Distant supervision is used to find the average yearly income of a user. We look for an occupational title of a user in the user's biography line. In case a user has multiple occupations, we use the first one we can find. With the found title, we look up the user's occupational class and the average hourly income for that class. We then label the user with the average yearly income by multiplying the average hourly income with the average total hours worked in The Netherlands. 

There are three additional data sources needed in order to make our annotation process work. 

First, we use a list of occupational titles and their respective classes from \citet{codelijsten} to look up the occupation of a user. These classes correspond with classes in the International Standard Classification of Occupations \citep{isco}, just like the earlier mentioned SOC. As this file was never meant for machinal consumption, the file had to be modified. All titles formed one long string, which had to be split in order to get the individual titles per class. Furthermore, the titles contained a lot of shorthand notations, e.g. "assistent-, coach" for the similar occupations "coach" and "assistent-coach" and slashes for synonyms like "typist / tekstverwerker". These were removed by hand as there was no suitable way to do this correctly in an automated manner. 
Second, we use a list of occupational classes and their respective average incomes from \citet{uurlonen} to look up the average hourly income for a particular class. We use the two-digit classes, as the incomes for almost all of them is known\footnote{There were no average incomes available for the two-digit (submajor) groups 62, 82, 92 and 95. They are therefore left out of the rest of our research.}. For most three- and four-digit classes, incomes aren't provided by Statistics Netherlands. Furthermore, the incomes among the two-digit groups varies enough in order to create a viable two-class split of our data.
Finally, to derive the average yearly income we need to know the average worked hours per year in The Netherlands. According to the \citet{hours} the average Dutch worker makes 1677 hours a year.

To evaluate the performance of our distant supervision method a random survey of 100 users per class was taken. Their labels were manually checked in a two class setting as mentioned in the previous subsection. The labels were considered correct if they appeared in the biography of a user, the user was a human and the occupational title was used to indicate paying occupation, not a hobby or study. The accuracy over the whole group of 200 users was 68,5 percent, with 59 percent in the low class and 78 percent in the high class. In 33 cases, the labels were wrong because the account was simply not used by a person but by a company. As there is no surefire way to distinguish between human and non-human users, we disregard these cases. The overall accuracy without these cases is 82 percent. The last few cases that were wrongly labeled consist of hobbies or voluntary work labelled as an occupation (15 cases), occupational titles that got bodged during the transformation of the needed file (11 cases) and four miscellaneous cases, which consist of users describing their former occupation in a non-trivially detectable way.

These results confirm that our distant supervision method yields enough correctly labeled data for our research, even though it is far from perfect. Possible future improvements of the method will follow in the \textit{Future Work} section of \autoref{conclusion}.

% TODO: ADD LINKS TO CHAPTERS
% TODO: ADD SLOAN TO EVAL CRITERIA?

\section{Processing}
Concerning the users, after extracting users with a known occupation from the twitter2 corpus a number of processing steps are involved to bring the entire group of 36113 users to a manageable number of suitable users. First, users that no longer exist, have their profile set as private or have less than 1000 tweets are removed to ensure we can get enough data per user for our research. The users are checked by using the Lookup API of Twitter, which allows us to check users in batches of 100. The remaining users are saved in a Python dictionary and written to disk with the built-in Pickle library. Afterwards, the remaining users are divided in the chosen income classes. From each class 1500 users are randomly selected for further processing by using the random.choice function of the NumPy Python library \citep{numpy}. For these users, their latest 1000 tweets are collected. Retweets and non-Dutch tweets were discarded. The language classification of each tweet was performed by the langid Python library \citep{langid}. Users with less than 500 suitable tweets were left out of the data set. For all remaining users the tweets are written to a text file per user per class. From the remaining set of users, 1000 were randomly selected per group to be used in our research.

After the processing of the users is completed, the tweets are prepared for further use. URL's, hashtags and usernames are removed and the tweets are tokenized so that relevant features can be derived from them. The processing relies on the \textit{TweetTokenizer} included in the NLTK Python library \citep{nltk}, a popular library for natural language processing in Python. As some features need to be generated from text tokenized per sentence, all tweets were run through a pre-trained NLTK sentence tokenizer created by \citet{punkt}. It was trained on the Dutch part of the Multilingual Corpus 1 (ECI), particularly on articles from the "De Limburger" newspaper.The resulting data was collected in a dictionary and written to disk with the Pickle library.


\chapter{Method}
\citet{flekova} was a major source of inspiration for our methodology. However, as our research is limited in available time (7 weeks) and resources, we had to make some compromises in keep it workable, given all constraints. We mainly rely on the scikit-learn library \citep{sklearn} for the concrete implementation of our method. It provides a lot of tools for machine learning, from full blown classifiers and regression models to tools for evaluation. 

\section{Features}
Originally, we planned to implement all of the feature groups used in \citet{flekova}. Due to the constraints of our research, we could only adopt two of the four groups: surface and readability. To compensate for this, we added a third groups of features: n-grams. All groups will be discussed in more detail below.

\subsection{Surface}
From this group, we chose and implemented the following features:
\begin{itemize}
\item Length of a user's tweets in words
\item Length of a user's tweets in characters
\item Average word length in a user's tweets
\item Ratio of words longer than 5 characters in a user's tweets
\item Type-token ratio
\end{itemize}

As these features mostly rely on counting words and performing some basic calculations, they were relatively easy to implement. They are all based on tokenized tweets without punctuation. The last two features partly overlap with the group of readability features as they are considered predictors of readability \citep{flekova}.

\subsection{Readability}
\citet{flekova} used a host of readability measures. They all have some commonality in the way they are calculated, but differ in measuring scale and intended application. Instead of implementing each measure with all its peculiarities by hand, we relied on the readability library \citep{readability} for their calculation. This library provides a function that takes sentence tokenized text as its input and outputs the scores for several readability metrics. As sentence tokenization has already been performed by NLTK, we only need to provide the right input data for each user to calculate its scores. These texts do include punctuation symbols, as they are needed for the calculation of some measures.

The readability metrics included in our research are:
\begin{itemize}
\item Automated Readability Index
\item Coleman-Liau Index
\item Flesch-Kincaid Grade Level
\item Flesch Reading Ease
\item Gunning-Fog Index
\item LIX Index
\item SMOG Index
\end{itemize}

\subsection{N-grams}
For the final feature set, we looked at the syntax and style sets in the study by \citet{flekova}. 
The syntax set proved unimplementable in this short timeframe. Although the Alpino parser \citep{alpino} could provide us with the needed part-of-speech tags, it could not do so reliably. The parser would sometimes crash on input it could not handle.

We therefore moved on to the style set. Whilst some features like the ratio of words with numbers were easily implementable, others would prove more problematic. A lot of features depend on part-of-speech tags. Without a proper functioning parser we cannot implement those features at this moment. 
We had no other choice than looking elsewhere for a possible feature set to add.

Inspired by the work of fellow student Reinard van Dalen on classifying users based on their political preference, we decided to opt for word n-grams as our third and final feature set. We use a modified version of a function provided by our supervisor in order to generate the n-grams. It calls on NLTK's \textit{ngrams} function to compute all possible n-grams for a given text.

We chose to include unigrams, bigrams and trigrams in our research. They are counted in a binary fashion: the fact that a certain n-gram occurs in a user's text is recorded, but the amount of occurrences is not taken into account.

\section{Classification}
As stated in the previous chapter, incomes are not available for every occupation, but only for a select number of occupational classes. The machine learning task ahead of us should therefore be framed as classification, instead of regression.
Handling in the spirit of Flekova et al., we tried to find a classification method that is closest to the methods they have used, despite the difference in prediction task. They used linear regression and vector support regression with an RBF kernel as its non-linear counterpart for comparison. 

Two classification methods came to mind: a support vector machine (SVM) and logistic regression. We chose to only use the latter. Scikit-learn's documentation on SVM's states that "if the number of features is much greater than the number of samples, the method is likely to give poor performance" \citep{svm}.
As one of our feature sets is n-grams, one can expect thousands of features while the data set only consists of two thousand samples. We therefore shifted our focus to logistic regression and implemented it in our classifier using scikit-learn's \textit{LogisticRegression} module.

\section{Evaluation}
After implementing all features, the performance of the classifier and its composing features is assessed by using $k$-fold cross validation. In our case we apply a $k$ of 10, like \citet{flekova}.

For each fold, the results were printed to screen, including a list of most informative features. After each validation run the precision, recall and F1-scores are calculated in order to compare the different classifier setups. We test a host of different setups in order to find the most effective combination (i.e. giving us the highest F1-score with the least amount of features).  The tested feature compositions are shown in table \ref{evaluation-combinations}.

The classifiers will all be compared to a baseline. As the data set consists of two equally large groups, a majority baseline cannot be used. We therefore use a random baseline. Given that there are two classes and they are equal in size, the chance that a single instance belongs to a certain class is 50 percent ($Pcorrect = 0.5$). We rely on scikit-learn's \textit{StratifiedKFold} module to split our data for each of the ten folds. The stratification makes sure that the equality in class size in our data set will be carried through to the individual test and training sets and thus guarantees that our baseline will be the same on each run.

\begin{table}[]
\caption{An overview of the classifier setups included in our evaluation.}
\centering
\begin{tabular}{llll}
\hline
\#          & \textbf{Set 1} & \textbf{Set 2} & \textbf{Set 3}    \\ \hline
\textbf{1}  & Surface        & Readability    & N-grams (n=1-2-3) \\
\textbf{2}  & Surface        & Readability    & N-grams (n=2-3)   \\
\textbf{3}  & Surface        & Readability    & N-grams (n=1-3)   \\
\textbf{4}  & Surface        & Readability    & N-grams (n=1-2)   \\
\textbf{5}  & Surface        & Readability    & N-grams (n=3)     \\
\textbf{6}  & Surface        & Readability    & N-grams (n=2)     \\
\textbf{7}  & Surface        & Readability    & N-grams (n=1)     \\
\textbf{8}  & Surface        & Readability    & -                 \\
\textbf{9}  & Surface        & -              & N-grams (n=1-2-3) \\
\textbf{10} & Surface        & -                & N-grams (n=2-3)   \\
\textbf{11} & Surface        & -              & N-grams (n=1-3)   \\
\textbf{12} & Surface        & -              & N-grams (n=1-2)   \\
\textbf{13} & Surface        & -              & N-grams (n=3)     \\
\textbf{14} & Surface        & -              & N-grams (n=2)     \\
\textbf{15} & Surface        & -              & N-grams (n=1)     \\
\textbf{16} & Surface        & -              & -                 \\
\textbf{17} & -              & Readability    & N-grams (n=1-2-3) \\
\textbf{18} & -              & Readability    & N-grams (n=2-3)   \\
\textbf{19} & -              & Readability    & N-grams (n=1-3)   \\
\textbf{20} & -              & Readability    & N-grams (n=1-2)   \\
\textbf{21} & -              & Readability    & N-grams (n=3)     \\
\textbf{22} & -              & Readability    & N-grams (n=2)     \\
\textbf{23} & -              & Readability    & N-grams (n=1)     \\
\textbf{24} & -              & Readability    & -                 \\
\textbf{25} & -              & -              & N-grams (n=1-2-3) \\
\textbf{26} & -              & -                & N-grams (n=2-3)   \\
\textbf{27} & -              & -               & N-grams (n=1-3)   \\
\textbf{28} & -              & -               & N-grams (n=1-2)   \\
\textbf{29} & -              & -              & N-grams (n=3)     \\
\textbf{30} & -              & -                & N-grams (n=2)     \\
\textbf{31} & -              & -                & N-grams (n=1)    
\end{tabular}
\label{evaluation-combinations}
\end{table}


\chapter{Results and Discussion}
\begin{table}[]
\caption{An overview of precision, recall and F1-scores for the different classifier setups.}
\label{results-overview}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Setup} & \textbf{Precision (P)} & \textbf{Recall (R)} & \textbf{F1-score} \\ \midrule
\textbf{Baseline} & 0.50                   & 0.50                & 0.50              \\
\textbf{1}        & 0.72                   & 0.72                & 0.72              \\
\textbf{2}        & 0.69                   & 0.69                & 0.69              \\
\textbf{3}        & 0.70                   & 0.70                & 0.70              \\
\textbf{4}        & 0.71                   & 0.71                & 0.71              \\
\textbf{5}        & 0.66                   & 0.66                & 0.66              \\
\textbf{6}        & 0.70                   & 0.70                & 0.70              \\
\textbf{7}        & 0.69                   & 0.69                & 0.69              \\
\textbf{8}        & 0.69                   & 0.69                & 0.69              \\

\textbf{9}         & 0.71                   & 0.71                & 0.71              \\
\textbf{10}       & 0.70                   & 0.70                & 0.70              \\
\textbf{11}       & 0.70                   & 0.70                & 0.70              \\
\textbf{12}       & 0.71                   & 0.71                & 0.71              \\
\textbf{13}       & 0.67                   & 0.67                & 0.67              \\
\textbf{14}       & 0.71                   & 0.70                & 0.70              \\
\textbf{15}       & 0.70                   & 0.70                & 0.70              \\
\textbf{16}       & 0.56                   & 0.56                & 0.56              \\

\textbf{17}        & 0.59                   & 0.59                & 0.59              \\
\textbf{18}       & 0.72                   & 0.72                & 0.72              \\
\textbf{19}       & 0.71                   & 0.71                & 0.71              \\
\textbf{20}       & 0.72                   & 0.72                & 0.72              \\
\textbf{21}       & 0.68                   & 0.68                & 0.68              \\
\textbf{22}       & 0.69                   & 0.69                & 0.69              \\
\textbf{23}       & 0.70                   & 0.70                & 0.70              \\
\textbf{24}       & 0.57                   & 0.57                & 0.57              \\

\textbf{25}       & 0.72                   & 0.72                & 0.72              \\
\textbf{26}       & 0.70                   & 0.70                & 0.70              \\
\textbf{27}       & 0.71                   & 0.71                & 0.71              \\
\textbf{28}       & 0.72                   & 0.72                & 0.72              \\
\textbf{29}       & 0.68                   & 0.68                & 0.68              \\
\textbf{30}       & 0.69                   & 0.69                & 0.69              \\
\textbf{31}       & 0.70                   & 0.70                & 0.70              \\ \bottomrule
\end{tabular}
\end{table}

All evaluation results for the different classifier setups and our baseline method can be found in table \ref{results-overview}. All setups outperformed our baseline, but differ quite a lot in how much. We will discuss a few interesting findings below.

One interesting observation is that feature groups can be almost equal in performance when acting alone, but can strengthen one another when used together. A good example of this is the combination of surface and readability features. Alone, they show roughly equal performance ($F1_{\text{surface}} = 0.56$,  $F1_{\text{readability}} = 0.57$). As mentioned before, some surface features are also seen as indicative for readability, hence the similarity in performance. Both outperform our baseline method, but by just a little. When we combine them, however, the F1-score climbs to 0.69, which is well above our baseline and not that far away from the best performing setups in our test.

The opposite is also observed during our tests: combining surface features with uni-, bi- and trigrams results in an F1-score of 0.59, whereas that same combination without the readability features reached an F1-score of 0.72. It shows us that finding the right balance between features is crucial for developing a proper classifier. A possible explanation in this particular case is that the readability features and the trigrams provide contradicting information, which causes our performance to drop. This hypothesis is backed up by the performance of the same setup without trigrams: the F1-score in this case rises back to 0.72.

Now, for determining which classifier is the most effective. The are five setups that showed equal performance, all reaching an F1-score of 0.72. The setups are:
\begin{itemize}
\item Surface + Readability + N-grams (n=1-2-3)
\item Readability + N-grams (n=2-3)
\item Readability + N-grams (n=1-2)
\item N-grams (n=1-2-3)
\item N-grams (n=1-2)
\end{itemize}

As we prefer the most effective setup, the N-grams (n=1-2) setup provides the best performance with the least amount of features. Although N-grams produce a lot of individual features, they can be easily generated, in contrast with many other features. The wealth of individual features present in this setup is probably the reason it performed so well, despite them being very rudimentary data points without any inherent value.
Generating N-grams can however take some time. Given that our setup with only surface and readability features reached an F1-score of 0.69, which is very close to the top performing setups, this setup deserves special consideration. It can be a very interesting option in time or resource limited environments as it only uses twelve individual features, as opposed the thousands of individual features n-grams introduce.



\chapter{Conclusion}
\label{conclusion}
\section{Summary}
We started our research with the question: \textit{to what extent is it possible to accurately predict the income level of a Dutch Twitter user and which stylometric features are predictive?} Earlier work on English speaking users looked promising. \citet{flekova} concluded that differences in style could be used to "tailor the style of a document without altering its topic to either income or age individually". This matched our hopes for a potential application of this technique: tailoring customer service communication to customers without having to ask them for background information.

As there had not been any comparable study on Dutch users, we had to create a dataset from scratch. By employing the labeling strategy established by \citet{sloan}, we were able to create a corpus of 21862 users with known incomes. They were divided into two two income classes, one above and one below \euro 34.500. From each class thousand users were selected for further research. Their 500 latest tweets were preprocessed for use in our classifier.

In order to classify our users we employed a logisitic regression method provided by Scikit-learn \citep(sklearn) with three sets of features: surface, readability and n-gram features. After testing a host of different feature group combinations on our classifier we managed to get a maximum F1-score of 0.72 using 10-fold cross validation, beating our baseline F1 of 0.5. The feature set with only unigrams and bigrams proved the most effective, providing the highest F1 score with the least amount of features.

The set with surface and readability features combined does deserve special consideration. Although it does not perform as well as the unigrams and bigrams, it comes close with an F1 score of .69, using only twelve individual features. In enviroments were resources or time is limited, this setup may prove a very interesting option.

\section{Limitations}
As our study was limited in the time and resources available it is far from conclusive. We will describe a few of its limitation below.

\subsection{Availability of income data}
The first important limitation of our study is the level of detail in the income data used to label our users. As Statistics Netherlands does not provide incomes for all occupations, we were forced to use and predict income classes instead of individual incomes. By using the average incomes for each two-digit group there is a possibility that there is a large gap between the predicted income and the actual income of a user, which might lead to erroneous classifications. As we have no full income data, we cannot assess the size of the problem at this time.

\subsection{Labeling strategy}
Our labeling strategy worked reasonably well, as demonstrated by the random survey in \autoref{datagathering}. However, it is far from perfect. One problem is that is cannot distinguish between human and non-human users. Although this is such a complex topic that it would warrant a separate study, adding this capability would help to reduce the noise in our dataset. Another problem is that out labeling method relies on just one heuristic: it will always take the first occupation it can find, even if there are multiple occupations mentioned by a user. It currently has no way to disambiguate between multiple occupations. We will present possibilities for future research on our labeling strategy in the next section.

\subsection{Class and classifier setup}
We originally planned to use two different class setups: a two class setting with high and low classes and a six class setting used by Statistics Netherlands in their research (\euro 0 - \euro 10.000, \euro 10.000 - \euro 20.000, \euro 20.000 - \euro 30.000, \euro 30.000 - \euro 40.000, \euro 40.000 - \euro 50.000 and \euro 50.000 or more). As our data gathering and annotation efforts proved quite challenging, given that there was no prior gathering and annotation method for Dutch users, we decided to limit ourselves to the two class setting only.

For the same reason, we had to limit our exploration of different classifier setups. We planned to do a thorough review like\citet{flekova} where each individual feature was measured on its predictiveness using two different regression methods. We limited ourselves to one classification method, logistic regression, as the documentation of scikit-learn indicated that support vector machines would likely give poor performance on our feature sets \citep{svm}. This would have mainly affected setups with n-gram features. Exploration of different classification methods may prove worthwhile in improving results.

Regarding the included features, we only assessed the influence of different feature set combinations, including different levels of n-grams as they greatly influence the amount of features available to our classifier. This limited assessment already yielded 31 distinct feature group combinations to be evaluated. As this proved time consuming, more detailed evaluations had to be left out.

\section{Future work}
\label{futurework}
A mayor boost to future work on profile Dutch authors on income would be the availability of incomes on a full four-digit level so individual incomes can be predicted and not just income classes. However, as we depend on Statistics Netherlands for our income data, it is out of our hands. For now, there are three clear avenues for future research using the available data.

\subsection{Improvement of labeling method}
Although our current labeling method is a good basis, it could become more robust. Disambiguation between multiple occupations by selecting the one with the highest ISCO class would be one possibility for improving our labeling accuracy, as it would also limit the amount of hobbies, studies and volunteer's work falsely detected as occupations in cases where multiple candidate occupations are present.

\subsection{Different class setups}
As we only tested a two class setup, it would make sense to test the performance of our classifier on a data set with more than two classes. This would also bring the income classes to the actual income of the user, thus resulting in more valuable predictions. It would be interesting to see how the predictive power of certain feature sets hold up in a different situation.

\subsection{In-depth analysis of features}
As we only investigated the influence of entire feature groups on our classifier's performance, it might be worthwhile to research the influence of individual features in the surface and readability groups. It might become clear that an entire different mix of individual features yields better results than the best ones our current setup could provide.

\section{Final statement}
This study has been a first exploration of the possibilities of profiling Dutch authors on their income. We hope that this may be the basis of further research and, in combination with more detailed income data, be a stepping stone towards predicting the income of individual Dutch authors. 


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{chicago} 
\bibliography{thesis-IK}


%----------------------------------------------------------------------------------------
%	APPENDICES
%----------------------------------------------------------------------------------------
\newpage
\appendix
\chapter{Appendix}
\section{List of occupational classes and respective occupational titles}


\newpage
\section{List of average hourly incomes for each occupation class}

\newpage
\section{List of commands for using our toolset}
\textit{As I am still cleaning up my code base, I will add this list to the final version of my thesis.}
\end{document}



